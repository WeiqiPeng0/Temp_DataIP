{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5fc2bdf-fb12-42ad-8dc3-e03e805f26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import *\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from trainer import *\n",
    "from tqdm import tqdm\n",
    "from matchloss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54992a38-545b-46cb-bd07-5261cbb682aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "device = 'cuda'\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c222e20-9e73-4f33-b92f-28a553180ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dst_train, dst_test= load_cifar10_data()\n",
    "clean_train_loader, clean_test_loader= load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a71f838-0f2d-44fa-9984-a953e79395a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "images_all = []\n",
    "labels_all = []\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed1a30f-3391-4ccc-abec-527fc5c67ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_test = [torch.unsqueeze(dst_test[i][0], dim=0) for i in range(len(dst_test))]\n",
    "labels_test = [dst_test[i][1] for i in range(len(dst_test))]\n",
    "images_test = torch.cat(images_test, dim=0).to(device)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f0ad72-9bf8-4e4c-9e9c-0fd54b9c818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set -1 for selecting across classes\n",
    "# Otherwise set class number\n",
    "def get_images(n, c = -1): # get random n images from class c\n",
    "    idx_shuffle = None\n",
    "    if c >= 0:\n",
    "        idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "    else:\n",
    "        idx_shuffle = np.random.randint(0, len(images_all), n)\n",
    "    return idx_shuffle, images_all[idx_shuffle], labels_all[idx_shuffle]\n",
    "\n",
    "def get_noises(idxs, noise):\n",
    "    noises = [noise[i] for i in idxs]\n",
    "    return torch.stack(noises).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ba08e0d-5fdd-47d3-b0f5-53b577c1783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clsmodel = ResNet18().cuda();\n",
    "clsmodel.train(); \n",
    "clsmodel = torch.nn.DataParallel(clsmodel)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "clsoptimizer = optim.SGD(clsmodel.parameters(), lr=learning_rate,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(clsoptimizer, T_max=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e116b55-6085-4db1-b745-95925d03be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "# clsmodel.load_state_dict(checkpoint['net'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41925a-73b5-4984-8111-5a462589aada",
   "metadata": {},
   "source": [
    "Set a target for seed image. Initialize seed images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56d01eb-6b4f-4c16-b3ef-9666fa872f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "target = 8\n",
    "ipc = 1  # of seed imags\n",
    "image_syn = torch.tensor(torch.zeros(ipc, 3, 32, 32), dtype=torch.float, requires_grad=True, device='cuda')\n",
    "label_syn = torch.tensor([target for _ in range(ipc)], dtype=torch.long, requires_grad=False, device='cuda').view(-1)\n",
    "\n",
    "optimizer_img = torch.optim.SGD([image_syn, ], lr=0.5, momentum=0.5) # optimizer_img for synthetic data\n",
    "optimizer_img.zero_grad()\n",
    "# criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "\n",
    "net_parameters = list(clsmodel.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa729dd-9489-4be0-a45d-cc52800e838a",
   "metadata": {},
   "source": [
    "Select subset of samples for adding perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f47fedbc-1017-47df-a26e-69a82c148155",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_idx, perturb_images, perturb_labels = get_images(500, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e765f18-8b60-4d87-abf7-5e9695cd4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.zeros([50000, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c2d3a3-3493-42c7-a64b-b78d3102becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 18/196 [00:06<01:06,  2.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25422/2291947921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Acc: %.3f%% (%d/%d)'\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "condition = True\n",
    "step_size = 0.001\n",
    "step_size_sync = 0.001\n",
    "epsilon = 8/255\n",
    "epoch = 0\n",
    "I = 200\n",
    "J= 2\n",
    "N = len(images_all)\n",
    "\n",
    "while condition:\n",
    "    \n",
    "    if epoch != 0 and epoch % 3 == 0:\n",
    "        np.save( 'noise7',noise.numpy())\n",
    "        np.save( 'synimg7', image_syn.detach().cpu().numpy())\n",
    "        np.save( 'perturb_idx7', perturb_idx)\n",
    "        step_size_sync = max(step_size_sync / 2, 0.0003)\n",
    "    clsmodel.train()\n",
    "    \n",
    "    idx = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in tqdm(range(N//batch_size + 1)):\n",
    "        batch_noise = []\n",
    "        leftl, rightl = batch_size * i, min(batch_size * (i+1), N)\n",
    "        images, labels = images_all[leftl:rightl].cuda(), labels_all[leftl:rightl].cuda()\n",
    "        perturb_img = None\n",
    "        for i, _ in enumerate(images):\n",
    "            # Update noise to images\n",
    "            batch_noise.append(noise[idx])\n",
    "            idx += 1\n",
    "        batch_noise = torch.stack(batch_noise).cuda()\n",
    "\n",
    "        perturb_img = Variable(images + batch_noise, requires_grad = False)\n",
    "        perturb_img = Variable(torch.clamp(perturb_img, 0, 1), requires_grad=False)\n",
    "\n",
    "            # perturb_img = Variable(images, requires_grad = False)\n",
    "        clsmodel.train()\n",
    "        clsmodel.zero_grad()\n",
    "        clsoptimizer.zero_grad()\n",
    "        output = clsmodel(perturb_img)\n",
    "        clsloss = criterion(output, labels)\n",
    "        clsloss.backward()\n",
    "        clsoptimizer.step()\n",
    "        _, predicted = output.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    print('Training Acc: %.3f%% (%d/%d)'% (100.*correct/total, correct, total))\n",
    "    scheduler.step()\n",
    "   \n",
    "    clsmodel.eval()\n",
    "    \n",
    "    for c in range(I):\n",
    "   \n",
    "        # the target gradients can be computed at very start to avoid repetition\n",
    "        pred_ = clsmodel(image_syn)\n",
    "        loss_ = criterion(pred_, label_syn)\n",
    "        gw_syn = torch.autograd.grad(loss_, net_parameters, create_graph=True)\n",
    "        \n",
    "        orig_img = Variable(perturb_images, requires_grad = False)\n",
    "        perturb_img = Variable(perturb_images + get_noises(perturb_idx, noise), requires_grad = False)\n",
    "        perturb_img = Variable(torch.clamp(perturb_img, 0, 1), requires_grad=True)\n",
    "        perturb_img.retain_grad()\n",
    "        \n",
    "        # gradient of (x + delta)\n",
    "        pred = clsmodel(perturb_img)\n",
    "        loss = criterion(pred, perturb_labels)\n",
    "        gw_real = torch.autograd.grad(loss, net_parameters, create_graph = True)\n",
    "        \n",
    "        # new: gradient of (x)\n",
    "        pred_orig = clsmodel(orig_img)\n",
    "        loss_orig = criterion(pred_orig, perturb_labels)\n",
    "        gw_real_orig = torch.autograd.grad(loss_orig, net_parameters, create_graph = True)\n",
    "\n",
    "        matchloss = match_loss(gw_syn, gw_real, 'ours') \n",
    "        matchloss_orig = match_loss(gw_syn, gw_real_orig, 'ours')\n",
    "        \n",
    "        total_loss = matchloss - matchloss_orig\n",
    "        \n",
    "        progress_bar(c, I, \"Total Loss: {}  Classification loss: {}\".format(total_loss, loss_))\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        eta = step_size * perturb_img.grad.data.sign() * (-1)\n",
    "        perturb_img = Variable(perturb_img.data + eta, requires_grad=True)\n",
    "        eta = torch.clamp(perturb_img.data - perturb_images.data, -epsilon, epsilon)\n",
    "        perturb_img = Variable(perturb_images.data + eta, requires_grad=True)\n",
    "        perturb_img = Variable(torch.clamp(perturb_img, 0, 1), requires_grad=True)\n",
    "\n",
    "        eta = torch.clamp(perturb_img.data - perturb_images.data, -epsilon, epsilon)\n",
    "        for i, delta in enumerate(eta):\n",
    "            noise[perturb_idx[i]] = delta.clone().detach().cpu()\n",
    "        \n",
    "        image_syn = Variable(image_syn + step_size_sync * image_syn.grad.data.sign() * (-1), requires_grad = True)\n",
    "        image_syn = Variable(torch.clamp(image_syn, 0, 1),requires_grad = True)\n",
    "        \n",
    "    clsmodel.eval()\n",
    "    correct = 0\n",
    "    total = image_syn.shape[0]\n",
    "    for img in image_syn:\n",
    "        pred = clsmodel(img.unsqueeze(0))\n",
    "        _, lb = pred.max(1)\n",
    "        correct += lb == target\n",
    "        \n",
    "    print(f\"{correct.detach().cpu()[0]}/{total} sync images are in target class\")\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        test_data(clsmodel, images_test, labels_test, criterion, batch_size = 256)\n",
    "    \n",
    "            \n",
    "    epoch += 1 \n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883078e-fae7-43ff-abe5-de27d48e06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(noise.numpy(), 'noise.py')\n",
    "np.save(image_syn.detach().cpu().numpy(), 'synimg.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44227418-2b1a-4e6c-8464-ab8094d2be67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.705 | Acc: 82.080% (8208/10000)\n"
     ]
    }
   ],
   "source": [
    "test_data(clsmodel, images_test, labels_test, criterion, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41e46ad7-d709-482b-b32f-b268f3f33a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8], device='cuda:0')\n",
      "1/1 sync images are in target class\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = image_syn.shape[0]\n",
    "for img in image_syn:\n",
    "    pred = clsmodel(img.unsqueeze(0))\n",
    "    _, lb = pred.max(1)\n",
    "    print(lb)\n",
    "    correct += lb == target\n",
    "print(f\"{correct.detach().cpu()[0]}/{total} sync images are in target class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b9c47-6f87-4d57-a7de-9bef02357521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520a038-3286-4c05-a3b0-9aa81397641c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
