{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5fc2bdf-fb12-42ad-8dc3-e03e805f26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import *\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from trainer import *\n",
    "from tqdm import tqdm\n",
    "from matchloss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54992a38-545b-46cb-bd07-5261cbb682aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "device = 'cuda'\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c222e20-9e73-4f33-b92f-28a553180ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dst_train, dst_test= load_cifar10_data()\n",
    "clean_train_loader, clean_test_loader= load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a71f838-0f2d-44fa-9984-a953e79395a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# formatting all data\n",
    "images_all = []\n",
    "labels_all = []\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22f0ad72-9bf8-4e4c-9e9c-0fd54b9c818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images for certain class\n",
    "def get_images(n, c = 0): # get random n images from class c\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "    return idx_shuffle, images_all[idx_shuffle]\n",
    "\n",
    "# get noises at selected indexes\n",
    "def get_noises(idxs, noise):\n",
    "    noises = [noise[i] for i in idxs]\n",
    "    return torch.stack(noises).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ba08e0d-5fdd-47d3-b0f5-53b577c1783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clsmodel = ResNet18().cuda();\n",
    "clsmodel.train(); \n",
    "clsmodel = torch.nn.DataParallel(clsmodel)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "clsoptimizer = optim.SGD(clsmodel.parameters(), lr=learning_rate,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(clsoptimizer, T_max=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e116b55-6085-4db1-b745-95925d03be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using chackpoint? uncomment this\n",
    "\n",
    "# checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "# net.load_state_dict(checkpoint['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b56d01eb-6b4f-4c16-b3ef-9666fa872f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# target class for seed image\n",
    "target = 0\n",
    "ipc = 1  # of seed imags\n",
    "\n",
    "# initialize seed image(s)\n",
    "image_syn = torch.tensor(torch.zeros(ipc, 3, 32, 32), dtype=torch.float, requires_grad=True, device='cuda')\n",
    "label_syn = torch.tensor([target for _ in range(ipc)], dtype=torch.long, requires_grad=False, device='cuda').view(-1)\n",
    "\n",
    "optimizer_img = torch.optim.SGD([image_syn, ], lr=0.5, momentum=0.5) # optimizer_img for synthetic data\n",
    "optimizer_img.zero_grad()\n",
    "\n",
    "net_parameters = list(clsmodel.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f47fedbc-1017-47df-a26e-69a82c148155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for perturbation\n",
    "perturb_idx, perturb_images = get_images(512, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e765f18-8b60-4d87-abf7-5e9695cd4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.zeros([50000, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9c2d3a3-3493-42c7-a64b-b78d3102becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================>................................]  Step: 1s103ms | Tot: 1m48s | Total Loss: 3696.824462890625  Classification loss: 7.3344774246215 101/200  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21702/1287963958.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturb_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mnoise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperturb_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mimage_syn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_syn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstep_size_sync\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Optimization algorithm. See overleaf for reference\n",
    "\n",
    "condition = True\n",
    "step_size = 0.001\n",
    "step_size_sync = 0.01\n",
    "epsilon = 8/255\n",
    "epoch = 0\n",
    "I = 200\n",
    "J= 2\n",
    "N = len(images_all)\n",
    "\n",
    "while condition:\n",
    "    \n",
    "    if epoch != 0 and epoch % 3 == 0:\n",
    "        np.save( 'noise5',noise.numpy())\n",
    "        np.save( 'synimg5', image_syn.detach().cpu().numpy())\n",
    "        step_size_sync = max(step_size_sync / 2, 0.0005)\n",
    "    clsmodel.train()\n",
    "    \n",
    "    idx = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in tqdm(range(N//batch_size + 1)):\n",
    "        batch_noise = []\n",
    "        leftl, rightl = batch_size * i, min(batch_size * (i+1), N)\n",
    "        images, labels = images_all[leftl:rightl].cuda(), labels_all[leftl:rightl].cuda()\n",
    "        perturb_img = None\n",
    "        for i, _ in enumerate(images):\n",
    "            # Update noise to images\n",
    "            batch_noise.append(noise[idx])\n",
    "            idx += 1\n",
    "        batch_noise = torch.stack(batch_noise).cuda()\n",
    "\n",
    "        perturb_img = Variable(images + batch_noise, requires_grad = False)\n",
    "        perturb_img = Variable(torch.clamp(perturb_img, 0, 1), requires_grad=False)\n",
    "\n",
    "            # perturb_img = Variable(images, requires_grad = False)\n",
    "        clsmodel.train()\n",
    "        clsmodel.zero_grad()\n",
    "        clsoptimizer.zero_grad()\n",
    "        output = clsmodel(perturb_img)\n",
    "        clsloss = criterion(output, labels)\n",
    "        clsloss.backward()\n",
    "        clsoptimizer.step()\n",
    "        _, predicted = output.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    print('Training Acc: %.3f%% (%d/%d)'% (100.*correct/total, correct, total))\n",
    "    scheduler.step()\n",
    "   \n",
    "    clsmodel.eval()\n",
    "    \n",
    "    images, labels = images_real, labels_real\n",
    "    \n",
    "    for c in range(I):\n",
    "   \n",
    "         # the target gradients can be computed at very start to avoid repetition\n",
    "        pred_ = clsmodel(image_syn)\n",
    "        loss_ = criterion(pred_, label_syn)\n",
    "        gw_syn = torch.autograd.grad(loss_, net_parameters, create_graph=True)\n",
    "\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        perturb_img = Variable(perturb_images + get_noises(perturb_idx, noise), requires_grad = False)\n",
    "        perturb_img = Variable(torch.clamp(perturb_img, 0, 1), requires_grad=True)\n",
    "        perturb_img.retain_grad()\n",
    "\n",
    "        # gw_real = list((x.detach().clone() for x in gw_real))\n",
    "\n",
    "        pred = clsmodel(perturb_img)\n",
    "        loss = criterion(pred, labels)\n",
    "        gw_real = torch.autograd.grad(loss, net_parameters, create_graph = True)\n",
    "        # gw_real = list((x.detach().clone() for x in gw_real))\n",
    "\n",
    "        matchloss = match_loss(gw_syn, gw_real, 'ours') + 1 * loss_\n",
    "        \n",
    "        progress_bar(c, I, \"Total Loss: {}  Classification loss: {}\".format(matchloss, loss_))\n",
    "\n",
    "        matchloss.backward()\n",
    "        # img_optimizer.step()\n",
    "\n",
    "        eta = step_size * perturb_img.grad.data.sign() * (-1)\n",
    "        perturb_img = Variable(perturb_img.data + eta, requires_grad=True)\n",
    "        eta = torch.clamp(perturb_img.data - images.data, -epsilon, epsilon)\n",
    "        perturb_img = Variable(images.data + eta, requires_grad=True)\n",
    "        perturb_img = Variable(torch.clamp(perturb_img, 0, 1), requires_grad=True)\n",
    "\n",
    "        eta = torch.clamp(perturb_img.data - images.data, -epsilon, epsilon)\n",
    "        for i, delta in enumerate(eta):\n",
    "            noise[perturb_idx[i]] = delta.clone().detach().cpu()\n",
    "        \n",
    "        image_syn = Variable(image_syn + step_size_sync * image_syn.grad.data.sign() * (-1), requires_grad = True)\n",
    "        image_syn = Variable(torch.clamp(image_syn, 0, 1),requires_grad = True)\n",
    "        \n",
    "    clsmodel.eval()\n",
    "    correct = 0\n",
    "    total = image_syn.shape[0]\n",
    "    for img in image_syn:\n",
    "        pred = clsmodel(img.unsqueeze(0))\n",
    "        _, lb = pred.max(1)\n",
    "        correct += lb == target\n",
    "    print(f\"{correct.detach().cpu()[0]}/{total} sync images are in target class\")\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        test_(clsmodel, clean_test_loader, criterion)\n",
    "    \n",
    "            \n",
    "    epoch += 1 \n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883078e-fae7-43ff-abe5-de27d48e06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized noise and seed image\n",
    "\n",
    "np.save(noise.numpy(), 'noise.py')\n",
    "np.save(image_syn.detach().cpu().numpy(), 'synimg.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b9c47-6f87-4d57-a7de-9bef02357521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2a8312e-7d33-43e8-917c-3eac34d2f64b",
   "metadata": {},
   "source": [
    "## testing\n",
    "For testing the trained seed image. Just load in the .npy and retrain a model on the perturbed dataset.\n",
    "Use same/different architecture for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05173c73-bfc8-47aa-9ef7-6b08f52aa9da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
